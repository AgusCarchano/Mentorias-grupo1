{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DApFPJlAnZer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zljLX_rNnZev"
   },
   "outputs": [],
   "source": [
    "# Opción para ver todas las columnas del dataset en el notebook\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwaoEF6QnZey"
   },
   "outputs": [],
   "source": [
    "data = \"data/bank-additional-full.csv\"\n",
    "df = pd.read_csv(data, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKe3pMCpnZew"
   },
   "source": [
    "# Práctico 04: Aprendizaje Supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boZ8PSfTnZex"
   },
   "source": [
    "Para finalizar nuestro modelo, aplicaremos estrategias de sampling para dividir entre train y test y haremos crossvalidation sobre train. Realizaremos pruebas con varios clasificadores y evaluaremos los resultados con múltiples métricas. Por último calcularemos el feature importance y obtendremos conclusiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUu2EEnlnZex"
   },
   "source": [
    "## Objetivo del práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isutnJ4VnZey"
   },
   "source": [
    "### Train-Validation-Test\n",
    "(obtener del práctico anterior)\n",
    "- División del dataset en train/validation/test\n",
    "- Estratificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBmnvVJPnZez"
   },
   "outputs": [],
   "source": [
    "# Reemplazamos la columna y (target) por 1 y 0\n",
    "df.y = df.y.replace('yes', 1)\n",
    "df.y = df.y.replace('no', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HjPIPM8nZez"
   },
   "outputs": [],
   "source": [
    "#Diferenciamos los atributos del target\n",
    "X = df.drop(columns='y')\n",
    "y = df.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CarrvxpenZe0"
   },
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y)   #Dejamos un conjunto de test con el 20% de los casos \n",
    "#Dado que el dataset se encuentra desbalanceado (aprox. 11% del total de casos pertenece a la clase 1), empleamos el parámetro stratified en función del target (y)\n",
    "#De este modo, la muestra seleccionada a partir de la división sería representativa para las dos clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8r3L1zcnZe0"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abDV6qJ75yR_"
   },
   "outputs": [],
   "source": [
    "#Para hacer algunas pruebas sin hacer doble division\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6_KjyfknZe1"
   },
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "- Tratamiento de valores nulos\n",
    "- Estandarización\n",
    "- Encoding de variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2svohT6GnZe1"
   },
   "outputs": [],
   "source": [
    "class SelectColumnsTransformer():\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe59cnornZe2"
   },
   "source": [
    "#### Pipeline genérico para pre-procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "giAdXjHynZe2"
   },
   "outputs": [],
   "source": [
    "#Aplicamos las transformaciones previas a los conjuntos de Train y Validation\n",
    "\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()), #Solamente activamos esta línea cuando el clasificador requiere escalado de variables numéricas (SVM, SGD)\n",
    "                            ('pca', PCA(n_components=4))   #En los casos que usemos las componentes principales en lugar de las variables numéricas\n",
    "                             ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),     #Activamos en el caso que quisieramos imputar la categoría desconocido\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxmdYFO2nZe8"
   },
   "source": [
    "### Definición de métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GifGaktnZe8"
   },
   "source": [
    "Definiremos las métricas a utilizar:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- AUC\n",
    "- PRAUC  \n",
    "\n",
    "Además investigaremos como utilizar el classification report y confusion matrix. Adicionalmente, cómo usar crossvalidation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erl6q1x42KvZ"
   },
   "source": [
    "Dado que el problema se encuetra desbalanceado, la Accuracy no es la medida adecuada para el análisis. Las métricas más adecuadas para el análisis comparativos de los modelos son: Precision, Recall, F1, AUC, PRAUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7ScSRet2Y4L"
   },
   "source": [
    "Explicación de las métricas utilizadas a un stakeholder no técnico.\n",
    "\n",
    "El conjunto de datos con que contamos para el análisis presenta una mayor cantidad de casos pertenecientes al grupo de clientes que no contrataron el PF en la última campaña de marketing en relación a aquellos clientes que si lo contrataron, razón por la cual decimos que el dataset se encuentra desbalanceado. Esta situación implica que contamos con mayor información para caracterizar a quienes no contrarían el PF que aquella disponible para caracterizar al grupo de clientes que si contratarían el PF, es decir, aquellos que constituyen el objetivo para nosotros.\n",
    "\n",
    "En este caso, para evaluar de manera comparativa una serie de modelos predictivos y poder juzgar cuál/es de ellos son los mejores tenemos que ser cuidadosos al momento de definir las métricas. No es recomendable confiar en métricas que se concentren únicamente en la cantidad de casos que están bien clasificados ya que se le estaría dando una mayor predominancia a las clasificaciones dentro de la clase mayoritaria (clientes que no contratan PF).\n",
    "\n",
    "En cuanto a las métricas tenemos entonces: \n",
    "- La Precisión representa qué porcentaje de los clientes que nuestro modelo predice que van a convertir, efectivamente lo hacen.\n",
    "- La Recall representa qué porcentaje de los clientes que convierten son captados correctamente por nuestro modelo predicitivo. \n",
    "\n",
    "Dado que nos interesa tanto la proporción de clientes que efectivamente convierten así como también la proporción de clientes que convierten bien identificados por el modelo, vamos a optar por una métrica que balancea estas dos cuestiones: la F1. Esta métrica es la media armónica de las dos anteriores, y sirve para poder evaluar comparativamente una serie de modelos contemplando los dos criterios mencionados.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo-BN4CGnZe8"
   },
   "source": [
    "## Testeo con varios modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FP6YAl-nZe9"
   },
   "source": [
    "Realizaremos varios tests con diversos tipos de modelos, tanto aquellos de la librería scikit-learn, como otros que no pertencen a ella:\n",
    "- Decision Tree\n",
    "- SGD Classifier\n",
    "- Logistic regression  \n",
    "- SVM     \n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "Usaremos crossvalidation y compararemos con validation y test.\n",
    "\n",
    "Realizaremos también optimizaciones de hiperparámetros en busca de los mejores valores para las métricas, empleando tanto Grid Search como Randomizes Search (en los casos en que la búsqueda se vuelva muy compleja)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR7eyTQrAWsC"
   },
   "source": [
    "## **Modelos Analizados**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrO7isoHrTBV"
   },
   "source": [
    "#### **MODELO BASELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo estimado como baseline para el problema bajo análisis fue un Árbol de Decsisiones, que fue presentado en el Práctico 3 y se incluye abajo para tomar de referencia en la comparación con los nuevos modelos que se entrenarán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjibQnHGraz-"
   },
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Test\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "\n",
    "pipeline_numerico = Pipeline([\n",
    "                             ('select_numeric_columns', SelectColumnsTransformer(variables_numericas))     #Para este modelo no es necesario escalar las variables numéricas\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('imputer', SimpleImputer(strategy='most_frequent', missing_values = 'unknown')),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=0, criterion = 'gini', max_depth = 10, min_samples_leaf = 3, min_samples_split = 2, class_weight=\"balanced\")\n",
    "dt_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= dt_clf.predict(train)\n",
    "y_val_pred=dt_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos la matriz de confusión para interpretar con mayor claridad los resultados\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_val,y_val_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True)\n",
    "\n",
    "plt.title(\"Matriz de Confusión\")   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe recordar que el objetivo es maximizar los valores que se encuentran en la diagonal princial (observaciones correctamente clasificadas para cada clase) y minimizar los valores de la diagonal secundaria (errores de clasificación de cada clase). \n",
    "En este caso puntual, el objetivo sería minimizar la cantidad de observaciones en el cuadrante inferior izquierdo, que representa clientes que contrataron el PF pero fueron clasificados por el modelo como que no lo contratarían, y el cuadrante superior derecho, que representa clientes que no contrataron el PF pero fueron clasificados por el modelo como que si lo contratarían."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT3TWoGAnZe_"
   },
   "source": [
    "#### **SGDClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una primera instancia, entrenamos este modelo a partir de la misma selección de las variables originales, mientras que en una segunda instancia, con el objetivo de mejorar las métricas, se entrenó el modelo a partir de una selección de variables categóricas y las componentes principales que habían sido obtenidas a partir del análisis de componentes principales en el Práctico 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler())      \n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parameters = {'model__loss':['hinge', 'log','squared_loss'], 'model__learning_rate':['constant', 'optimal', 'invscaling', 'adaptive'], \n",
    "              'model__penalty': ['l2', 'l1', 'elasticnet'], 'model__eta0': [ 1e-6,0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('model',SGDClassifier( random_state= 1))])\n",
    "sgd_clf = GridSearchCV(pipeline, parameters, scoring=('f1','roc_auc'), cv = 5,return_train_score=True, refit='f1')\n",
    "\n",
    "sgd_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display ('Best configuraton:')\n",
    "display(sgd_clf.best_params_)\n",
    "best_sgd_clf = sgd_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf=SGDClassifier( eta0=0.1, learning_rate='constant', loss='squared_loss', penalty='l1', random_state= 1)\n",
    "sgd_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= sgd_clf.predict(train)\n",
    "y_val_pred=sgd_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente, se entrenó el modelo con variables categóricas y las variables obtenidas por medio del análisis de componentes principales sobre las variables numéricas. Abajo se incluye el modelo que presentó las mejores métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABdbVOiGCnpU"
   },
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m',\"emp.var.rate\",\"cons.price.idx\",\"nr.employed\",'cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJk0758LnZe_"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parameters = {'model__loss':['hinge', 'log','squared_loss'], 'model__learning_rate':['constant', 'optimal', 'invscaling', 'adaptive'], \n",
    "              'model__penalty': ['l2', 'l1', 'elasticnet'], 'model__eta0': [ 1e-6,0.001, 0.01, 0.1, 1, 10]}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('model',SGDClassifier( random_state= 1))])\n",
    "clf = GridSearchCV(pipeline, parameters, scoring=('f1','roc_auc'), cv = 5,return_train_score=True, refit='f1')\n",
    "\n",
    "sgd_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "id": "rXmHRdAwnZe_",
    "outputId": "eef2aaf9-83f2-410a-c82a-cf1906940d00"
   },
   "outputs": [],
   "source": [
    "display ('Best configuraton:')\n",
    "display(sgd_clf.best_params_)\n",
    "\n",
    "best_sgd_clf = sgd_clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sgd_clf=SGDClassifier(eta0= 1, learning_rate= 'constant', loss= 'log', penalty= 'l1', random_state= 1))\n",
    "best_sgd_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhfay_oqnZfA",
    "outputId": "7a3783ce-74a9-455d-9735-11feda618b1e"
   },
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= best_sgd_clf.predict(train)\n",
    "y_val_pred=best_sgd_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas obtenidas a partir del modelo empleando las 4 primeras PCA mejoran, pero sigue sin ofreccer buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             #('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf=LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "lr_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= lr_clf.predict(train)\n",
    "y_val_pred=lr_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con PCA NO MEJORA\n",
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf=LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "lr_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= lr_clf.predict(train)\n",
    "y_val_pred=lr_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             #('standard_scaler', StandardScaler()),\n",
    "                             #('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf=SVC(random_state=0, class_weight=\"balanced\")\n",
    "svc_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= svc_clf.predict(train)\n",
    "y_val_pred=svc_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con PCA NO MEJORA\n",
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf=SVC(random_state=0, class_weight=\"balanced\")\n",
    "svc_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= svc_clf.predict(train)\n",
    "y_val_pred=svc_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jG5rDkv5hCx"
   },
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el modelo anterior, entrenamos el modelo en primera instancia con las variables originales y luego probamos contemplando las variables que surgieron del PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QnIxWgaCylg"
   },
   "outputs": [],
   "source": [
    "#Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([\n",
    "                             ('select_numeric_columns', SelectColumnsTransformer(variables_numericas))     #Para este modelo no es necesario escalar las variables numéricas\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                 ('imputer', SimpleImputer(strategy='most_frequent', missing_values = None)),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 6, 10, None],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf':[1,2,3]\n",
    "}\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "cv_forest_clf = GridSearchCV(forest_clf, search_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "cv_forest_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display ('Best configuraton:')\n",
    "display(cv_forest_clf.best_params_)\n",
    "best_forest_clf = cv_forest_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= best_forest_clf.predict(train)\n",
    "y_val_pred=best_forest_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojgo2oSJ4AZG"
   },
   "source": [
    "Al igual que en el modelo anterior, entrenamos empleando las 4 primeras componentes que surgen del PCA sobre las variables numéricas y encontramos mejoras en las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kmx9GKLr3yMg"
   },
   "outputs": [],
   "source": [
    "##Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                 ('imputer', SimpleImputer(strategy='most_frequent', missing_values = None)),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxzXsIjy5pXb",
    "outputId": "c96dc989-dae2-41d8-bc1a-c6cba4fbbf1b"
   },
   "outputs": [],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vVgHv2v5efo",
    "outputId": "ed0bdf05-60d9-44f0-88fb-0b20b421f9d2"
   },
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [3, 6, 10, None],\n",
    "    'min_samples_split': [2, 3, 4],\n",
    "    'min_samples_leaf':[1,2,3]\n",
    "}\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "cv_forest_clf = GridSearchCV(forest_clf, search_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "cv_forest_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDXG4JmBlfAf"
   },
   "outputs": [],
   "source": [
    "display ('Best configuraton:')\n",
    "display(cv_forest_clf.best_params_)\n",
    "best_forest_clf = cv_forest_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mejores parámetros encontrados para RF**, obtenidos en otra notebook. Los pegamos acá. \n",
    "\n",
    "'Best configuraton:'\n",
    "{'criterion': 'entropy',\n",
    " 'max_depth': 15,\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 3,\n",
    " 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest_clf=RandomForestClassifier(criterion='entropy', max_depth=15, min_samples_leaf=1, min_samples_split=3, n_estimators=100, random_state=42)\n",
    "best_forest_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfRVhzleg6Jl",
    "outputId": "78b332e0-69ac-44cf-c1c5-1af12c230904"
   },
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= best_forest_clf.predict(train)\n",
    "y_val_pred=best_forest_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "affYLTnx-TOK"
   },
   "source": [
    "#### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el entrenamiento del Árbol de decisión a partir de las 4 primeras componentes principales y una selección de las variables categóricas, para chequear si esta opción mejora las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('imputer', SimpleImputer(strategy='most_frequent', missing_values = None)),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xh8NOi93-YEF"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parameters = {'criterion':['gini', 'entropy'], 'max_depth':[None, 3, 5, 10, 20, 40], 'min_samples_split': [2,3,4], 'min_samples_leaf':[1,2,3,4]}\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "cv_dt_clf = GridSearchCV(tree, parameters, scoring=('f1','roc_auc'), cv = 5,return_train_score=True, refit='f1')\n",
    "\n",
    "cv_dt_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display ('Best configuraton:')\n",
    "display(cv_dt_clf.best_params_)\n",
    "best_dt_clf = cv_dt_clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DT Best Configurationn**, obtenido en otra notebook.\n",
    "\n",
    "'Best configuraton:'\n",
    "{'criterion': 'entropy',\n",
    " 'max_depth': 10,\n",
    " 'min_samples_leaf': 3,\n",
    " 'min_samples_split': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No me dio igual a la optimización que había hecho Santi y esta opción óptima overfitea el train\n",
    "best_dt_clf=DecisionTreeClassifier(criterion='gini', max_depth=20, min_samples_leaf=1, min_samples_split=2,random_state=0)\n",
    "best_dt_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecimos y obtenemos las métricas de este modelo\n",
    "y_train_pred= best_dt_clf.predict(train)\n",
    "y_val_pred=best_dt_clf.predict(val)\n",
    "\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"VALIDACIÓN\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BULyWdkpnY9C"
   },
   "source": [
    "#### **Naive bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNxPS2QNkTDY"
   },
   "outputs": [],
   "source": [
    "variables_categoricas = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "variables_numericas = ['age', 'campaign', 'previous', 'cons.conf.idx', 'euribor3m']\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             # ('standard_scaler', StandardScaler()),\n",
    "                             (\"kbins_discretizer\", KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"uniform\")),   #strategy=\"uniform\"\n",
    "                             ('bins_cat', OneHotEncoder())\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                 #('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),      #podríamos no ponerlo, y que deje \"desconocido\" como una categoría más\n",
    "                                 ('cat', OneHotEncoder())\n",
    "                                 ])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                   ('cat', pipeline_categorico, variables_categoricas),\n",
    "                                  ])\n",
    "\n",
    "pipeline_modelo = Pipeline([('preprocess', pipeline_completo),\n",
    "                            ('nb', ComplementNB())])\n",
    "\n",
    "#The Complement Naive Bayes classifier was designed to correct the “severe assumptions” made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.\n",
    "#En el pre-procesamiento transformé todos los atributos en categóricos, porque es el requerimiento del tipo de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-F8hB7FkYbw"
   },
   "outputs": [],
   "source": [
    "#Solo Pre-procesamiento\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8hZdetakdCX",
    "outputId": "88e6dbcf-5dec-43a7-88d7-1b26ae5ad81d"
   },
   "outputs": [],
   "source": [
    "#Modelo más sencillo\n",
    "nb=ComplementNB()\n",
    "nb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogf6EaZ5kgvL",
    "outputId": "ea764893-0ff4-42f1-ba60-9c24b633caba"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, nb.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, nb.predict(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q677M0Kkmko"
   },
   "outputs": [],
   "source": [
    "#Grilla de parámetros para optimización\n",
    "params={'alpha':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.3, 1.5],\n",
    "        'fit_prior':[True, False],\n",
    "        'norm':[True,False]\n",
    "       }\n",
    "\n",
    "nb=ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odRyJHVckrr7",
    "outputId": "f09109d1-c825-4402-ee7a-5c3d41be8aeb"
   },
   "outputs": [],
   "source": [
    "#Búsqueda de los mejores parámetros\n",
    "cv_nb = GridSearchCV(nb, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
    "cv_nb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17185s_wkxE2",
    "outputId": "b6872f11-ef9f-4b6e-bdfd-3284899cc0c5"
   },
   "outputs": [],
   "source": [
    "cv_nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXyMNRXCkz0s",
    "outputId": "aabd3bf9-00d8-4cea-eb63-e48f9c56207a"
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de la mejor versión encontrada del modelo\n",
    "nb_best = ComplementNB(alpha=0.3, fit_prior=True, norm=True)\n",
    "nb_best.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeolnXIik148",
    "outputId": "e2092023-2ff2-43ed-a4a3-c8c77697ddeb"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, nb_best.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, nb_best.predict(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CTHIWJvHS-a"
   },
   "source": [
    "En la mejor especificación encontrada para este tipo de modelos, y sobre el conjunto de validación, la F1 de la clase minoritaria es 0.39 y la F1 promedio entre las dos clases es 0.65. \n",
    "\n",
    "Dentro de la clase minoritaria tenemos que el 36% de los clientes que el modelo indica que contratarían el PF están identificados de manera correcta, y logra capturar al 43% de los clientes que efectivamente contrarían el PF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rldLCTDl32W"
   },
   "source": [
    "#### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGElyCK0nu0O"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJFPo3GZl3Pj"
   },
   "outputs": [],
   "source": [
    "variables_categoricas = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "variables_numericas = ['age', 'campaign', 'previous', 'cons.conf.idx', 'euribor3m']\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                 #('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),      #podríamos no ponerlo, y que deje \"desconocido\" como una categoría más\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                   ('cat', pipeline_categorico, variables_categoricas),\n",
    "                                  ])\n",
    "\n",
    "pipeline_modelo = Pipeline([('preprocess', pipeline_completo),\n",
    "                            ('xgb', xgb.XGBClassifier(seed=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGzVBzs3mMST"
   },
   "outputs": [],
   "source": [
    "#Solo Pre-procesamiento\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7WBCwU5mSub",
    "outputId": "73fc2b9b-c139-4776-d06c-e827b7295076"
   },
   "outputs": [],
   "source": [
    "xgb=xgb.XGBClassifier(seed=0)\n",
    "xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eePmzFAzmkcS",
    "outputId": "368e1c1e-09c7-4e4b-f039-71dbff7ecfd7"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, xgb.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, xgb.predict(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duAEjUzgmqxT"
   },
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros\n",
    "#Grilla de parámetros\n",
    "params={'objective':[\"binary:logistic\",\"binary:hinge\",\"binary:logitraw\"],\n",
    "        'learning_rate':[ 0.1,0.2,0.3],\n",
    "        'max_depth':[2,4, 6, 7, 8, 10],\n",
    "        'alpha':[2, 3, 5, 7],\n",
    "        \"n_estimators\":[5, 7, 10]\n",
    "       }\n",
    "       \n",
    "xg=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "atBORt9Gm-iX",
    "outputId": "55d177cd-92db-4c76-baf3-642f11d94074"
   },
   "outputs": [],
   "source": [
    "#Búsqueda de parámetros\n",
    "cv_xgb = GridSearchCV(xg, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
    "cv_xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFXg8QhfnUtV",
    "outputId": "23d62d22-317b-439c-d5a4-f3fa5f17f2e3"
   },
   "outputs": [],
   "source": [
    "cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgL20Q1-y7hd"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mznSQCahnakS",
    "outputId": "8ec43905-c200-43df-977f-750ecd0d912c"
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de la mejor versión encontrada del modelo en otra notebook\n",
    "xgb_best = xgb.XGBClassifier(seed=0, alpha= 2, learning_rate= 0.1, max_depth= 7, n_estimators=7, objective='binary:hinge')\n",
    "xgb_best.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmGT4nG5npUY",
    "outputId": "339b598a-5f59-46b7-8b38-148931d57048"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, xgb_best.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, xgb_best.predict(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnFxQKVmNmJc"
   },
   "source": [
    "En este caso, el mejor modelo encontrado sobre el conjunto de validación tiene una F1 igual a 0.48 en la clase minoritaria y una F1 promedio de las dos clases de 0.71.\n",
    "\n",
    "Dentro de la clase minoritaria tenemos que el 44% de los clientes que el modelo indica que contratarían el PF están identificados de manera correcta, y logra capturar al 54% de los clientes que efectivamente contrarían el PF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vyZAz1jx2Zh"
   },
   "outputs": [],
   "source": [
    "#Probamos con una variedad de combinaciones de variables, para validar cuáles son las más apropiadas para el modelo.\n",
    "#La mejor combinación de variables fue esta:\n",
    "variables_categoricas = ['job', 'education', 'contact','loan', 'day_of_week', 'poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous', 'cons.conf.idx', 'euribor3m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                 #('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),      #podríamos no ponerlo, y que deje \"desconocido\" como una categoría más\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                   ('cat', pipeline_categorico, variables_categoricas),\n",
    "                                  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo Pre-procesamiento\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros\n",
    "#Grilla de parámetros\n",
    "params={'objective':[\"binary:logistic\",\"binary:hinge\",\"binary:logitraw\"],\n",
    "        'learning_rate':[ 0.1,0.2,0.3],\n",
    "        'max_depth':[2,4, 6, 7, 8, 10],\n",
    "        'alpha':[2, 3, 5, 7],\n",
    "        \"n_estimators\":[5, 7, 10]\n",
    "       }\n",
    "       \n",
    "xg=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Búsqueda de parámetros\n",
    "cv_xgb = GridSearchCV(xg, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
    "cv_xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento de la mejor versión encontrada del modelo en otra notebook\n",
    "xgb_best = xgb.XGBClassifier(seed=0, alpha= 2, learning_rate= 0.1, max_depth= 10, n_estimators=7, objective='binary:hinge')\n",
    "xgb_best.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, xgb_best.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, xgb_best.predict(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYlDZ3r71oms"
   },
   "source": [
    "Alternativamente, probamos con optimizaciones aleatorias de parámetros y ofrecer mayor variabilidad a los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BK0_ZIVw1tX_"
   },
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros\n",
    "#Grilla de parámetros\n",
    "params={'objective':[\"binary:logistic\",\"binary:hinge\",\"binary:logitraw\"],\n",
    "        'learning_rate':[0.05,0.1,0.15,0.2,0.25,0.3,0.4,0.5],\n",
    "        'max_depth':[2,3, 4,5, 6, 7, 8, 9, 10, 12, 15],\n",
    "        'alpha':[0, 0.5, 1, 2, 3, 5],\n",
    "        'lambda':[0.5, 1, 2, 3, 5],\n",
    "        \"n_estimators\":[3, 5, 6, 7, 8, 9, 10, 15],\n",
    "        \"booster\":[\"gbtree\",\"dart\"],\n",
    "        \"gamma\":[0.5,1,2,5, 7, 8],\n",
    "        \"tree_method\":[\"auto\",\"exact\",\"approx\",\"hist\"]\n",
    "       }\n",
    "xg=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTVb4wbs1yOz",
    "outputId": "e555bee6-cd65-44a1-fded-418e7d3539fc"
   },
   "outputs": [],
   "source": [
    "#Búsqueda de parámetros\n",
    "rcv_xgb = RandomizedSearchCV(xg, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
    "rcv_xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CIcLeHc14vr",
    "outputId": "c9c676e4-b1b1-4a5b-a42a-a8ccbf6239dc"
   },
   "outputs": [],
   "source": [
    "rcv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvvA9l4D15KZ",
    "outputId": "2519004b-3c02-456e-ac27-f826a718a3f3"
   },
   "outputs": [],
   "source": [
    "#Entrenamiento de la mejor versión encontrada del modelo\n",
    "xgb_best_r = xgb.XGBClassifier(seed=0, alpha= 0, booster=\"dart\", gamma= 0.5, reg_lambda=5, learning_rate= 0.05, \n",
    "                               max_depth= 7,n_estimators=15, objective=\"binary:hinge\", tree_method= 'hist'   \n",
    "                                )   #el lambda por default es 1\n",
    "xgb_best_r.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxKT-U0G19-S",
    "outputId": "f6e927ce-af71-4ae7-fc0c-dff47bc4e808"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, xgb_best_r.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, xgb_best_r.predict(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOupgG-9R3tx"
   },
   "source": [
    "Probamos alternativamente con las primeras 4 componentes principales del PCA sobre las variables numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('imputer', SimpleImputer(strategy='most_frequent', missing_values = None)),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de hiperparámetros\n",
    "#Grilla de parámetros\n",
    "params={'objective':[\"binary:logistic\",\"binary:hinge\",\"binary:logitraw\"],\n",
    "        'learning_rate':[ 0.1,0.2,0.3],\n",
    "        'max_depth':[2,4, 6, 7, 8, 10],\n",
    "        'alpha':[2, 3, 5, 7],\n",
    "        \"n_estimators\":[5, 7, 10]\n",
    "       }\n",
    "       \n",
    "xg=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Búsqueda de parámetros\n",
    "cv_xgb = GridSearchCV(xg, params, scoring='f1', cv=5,refit=True,n_jobs=-1)     \n",
    "cv_xgb.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento de la mejor versión encontrada del modelo\n",
    "xgb_best= xgb.XGBClassifier(seed=0, alpha= 5, learning_rate= 1, \n",
    "                               max_depth= 10,n_estimators=7, objective=\"binary:hinge\"   \n",
    "                                )   #el lambda por default es 1\n",
    "xgb_best.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, xgb_best.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, xgb_best.predict(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw4vt23dSaq-"
   },
   "source": [
    "##NO PUDE REPLICAR ESTE RESULTADO\n",
    "\n",
    "Comentario:\n",
    "\n",
    "El mejor modelo encontrado usando como variables prpedictoras las 4 PCA, me da lo siguiente: \n",
    "\n",
    "MÉTRICAS CONJUNTO DE TRAIN\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.94      0.95     22472\n",
    "           1       0.57      0.72      0.63      2587\n",
    "    accuracy                           0.91     25059\n",
    "   macro avg       0.77      0.83      0.79     25059\n",
    "weighted avg       0.93      0.91      0.92     25059\n",
    "\n",
    "MÉTRICAS CONJUNTO DE VALIDACIÓN\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.93      0.94      5618\n",
    "           1       0.51      0.65      0.58       647\n",
    "    accuracy                           0.90      6265\n",
    "   macro avg       0.74      0.79      0.76      6265\n",
    "weighted avg       0.91      0.90      0.91      6265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE4sX7QjRiFZ"
   },
   "source": [
    "#### **LGMB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XB7dgkHTaUy1"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG_8uvjEaXk2"
   },
   "outputs": [],
   "source": [
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]     \n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler())      \n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('select_categoric_columns', SelectColumnsTransformer(variables_categoricas)),\n",
    "                                  ('imputer', SimpleImputer(strategy='most_frequent', missing_values=\"unknown\")),\n",
    "                                 ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdnPX7vrabNH",
    "outputId": "3801e438-4425-4988-9744-f13d17b185a6"
   },
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier()\n",
    "lgb_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FLDMEJuwalDa",
    "outputId": "0de9d084-ba51-4078-cbc8-ca93a5bb4d0f"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, lgb_clf.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, lgb_clf.predict(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPLORAR UN POCO MÁS ESTE MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-CpMIvUanS0",
    "outputId": "8a1b296d-c3dc-4254-f8d0-e5f17f874277"
   },
   "outputs": [],
   "source": [
    "clf2 = lgb.LGBMClassifier(class_weight={0: 10, \n",
    "                1: 1})\n",
    "clf2.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yklWVvCzaqce",
    "outputId": "721767ff-e023-4337-ac66-013246cd0856"
   },
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, clf2.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, clf2.predict(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimización de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el entrenamiento de este modelo usando las 4 primeras componentes principales y las variables categóricas para ver si mejoran las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#con PCA NO MEJORA\n",
    "##Pre-procesamiento sobre los conjuntos de Train y Validation\n",
    "variables_categoricas = ['job', 'marital','education','housing', 'loan','contact','poutcome']\n",
    "variables_numericas = ['age', 'campaign','previous','euribor3m','cons.conf.idx' ]\n",
    "\n",
    "# Filtramos las variables que seleccionamos\n",
    "X_t = X_train[variables_categoricas + variables_numericas]\n",
    "X_v = X_val[variables_categoricas + variables_numericas]\n",
    "\n",
    "pipeline_numerico = Pipeline([('select_numeric_columns', SelectColumnsTransformer(variables_numericas)),\n",
    "                             ('standard_scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=4))\n",
    "                            ])\n",
    "\n",
    "pipeline_categorico = Pipeline ([('imputer', SimpleImputer(strategy='most_frequent', missing_values = None)),\n",
    "                                   ('cat', OneHotEncoder())])\n",
    "\n",
    "pipeline_completo = ColumnTransformer([('num', pipeline_numerico, variables_numericas),\n",
    "                                       ('cat', pipeline_categorico, variables_categoricas)\n",
    "                                      ])\n",
    "\n",
    "train = pipeline_completo.fit_transform(X_t)\n",
    "val = pipeline_completo.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier()\n",
    "lgb_clf.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MÉTRICAS CONJUNTO DE TRAIN\")\n",
    "print(classification_report(y_train, lgb_clf.predict(train)))\n",
    "print(\"MÉTRICAS CONJUNTO DE VALIDACIÓN\")\n",
    "print(classification_report(y_val, lgb_clf.predict(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwjW60pZnZfC"
   },
   "source": [
    "### Feature importance y explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5PljSOldxh0"
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRdmMVs6eGGA"
   },
   "outputs": [],
   "source": [
    "best_dt_clf = DecisionTreeClassifier(criterion='entropy', max_depth=10,\n",
    "                                        min_samples_leaf=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUb-LVGhnZfD",
    "outputId": "1d3a24d9-136f-439d-ab79-24bdfb24caa6"
   },
   "outputs": [],
   "source": [
    "#Corremos el pipeline con los parámetros del mejor modelo econtrado:\n",
    "\n",
    "pipeline_modelo_dt = Pipeline([('preprocess', pipeline_completo),\n",
    "                            ('rf', best_dt_clf)])\n",
    "pipeline_modelo_dt.fit(X_t, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzcgK8-0nZfE"
   },
   "source": [
    "#### Obtener los nombres de las variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5MK9jY5nZfE"
   },
   "outputs": [],
   "source": [
    "# Si realizamos one hot encoding, vamos a tener el problema de que se incrementan el numero de features y necesitamos la nueva lista.\n",
    "numeric_features = variables_pca\n",
    "cat_features = pipeline_modelo_dt.named_steps['preprocess'].transformers_[1][1][1].get_feature_names(variables_categoricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hLGOPBleRJ0"
   },
   "outputs": [],
   "source": [
    "onehot_columns = np.array(cat_features)\n",
    "numeric_features_list = np.array(numeric_features)\n",
    "numeric_features_list = np.append(numeric_features_list, onehot_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "BJlxQspXeTrw",
    "outputId": "e7604082-c98e-463b-cc43-bc42b820c10a"
   },
   "outputs": [],
   "source": [
    "# Es necesario ordenar las los valores del feature importance (utilizamos argsort para tener el orden de los indices)\n",
    "sorted_idx = pipeline_modelo_dt[1].feature_importances_.argsort()\n",
    "plt.figure(figsize = (18,9))\n",
    "plt.barh(numeric_features_list[sorted_idx], pipeline_modelo_dt[1].feature_importances_[sorted_idx])\n",
    "plt.xlabel(\"Decision Tree Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SMfSyoM6qBV"
   },
   "source": [
    "#### Graficar las variables originales que tuvieron más peso en el PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "id": "HH_KBM8D7eyX",
    "outputId": "1adfab3d-49cd-4c55-a256-066af80ff5e7"
   },
   "outputs": [],
   "source": [
    "pca = pipeline_completo.transformers_[0][1][2]\n",
    "\n",
    "n_pcs= pca.components_.shape[0]\n",
    "initial_feature_names = X_t[variables_numericas].columns\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in     range(n_pcs)]\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "zipped_feats = zip(most_important_names, pipeline_modelo_dt[1].feature_importances_)\n",
    "zipped_feats = sorted(zipped_feats, key=lambda x: x[1], reverse=True)\n",
    "features, importances = zip(*zipped_feats)\n",
    "top_features = features[:5]\n",
    "top_importances = importances[:5]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Feature Importances for PCA ')\n",
    "plt.barh(range(len(top_importances)), top_importances, color='b', align='center')\n",
    "plt.yticks(range(len(top_importances)), top_features)\n",
    "plt.xlabel('Relative Importance in PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36DkxhP1BHbE",
    "outputId": "2de5f517-bfab-4e9a-f878-1cf5f6e9a4a3"
   },
   "outputs": [],
   "source": [
    "most_important"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mentoria_Practico_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
